#!/usr/bin/python
__author__ = 'b0xr00ter'

import requests
from bs4 import BeautifulSoup
from urlparse import urlparse

print '''
            **************************************************************
             Welcome to the Web Crawler of Python Vulnerability Assesment Framework
                                            by Shivam Mehta
            **************************************************************'''



url = raw_input('Enter the url of where u want to search: ')   #"http://192.168.1.11"
parsed_url = urlparse(url)
data  = { "login" : "cust" ,
          "password": "cust" ,
          "btnogin": "Login"}
s = requests.Session()
response = s.post(url, data=data)
print response.status_code
print "My Accounts" in response.content

response=s.get(url)
soup = BeautifulSoup(response.text)
list_url = []
for lists in soup.find_all('a'):
    list_url.append(parsed_url.scheme+"://"+parsed_url.netloc+"/"+lists.get('href',""))


for i in list_url:
    if not 'logout' in i:
        response = s.get(i)
        soup = BeautifulSoup(response.text)
        for link in soup.find_all('a'):
            if not link.get('href')== 'logout.php':
                link = parsed_url.scheme+"://"+parsed_url.netloc+"/"+link.get('href', "")
                if not link in list_url:
                    list_url.append(link)

print len(list_url)
#print list_url
f = open("list.txt","w")
f.write("\n".join(list_url))
f.close()


for newurl in list_url:
    print newurl